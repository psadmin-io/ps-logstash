input {
   beats {
      type => beats
      port => 5044
   }
}

filter {

  # Drop empty log lines
  if [message] == "" {
    drop { }
  }

  # Web Server Access Logs
  if [document_type] == "access_log" {
    # drop comment lines
    if ([message] =~ /^#/) {
      drop{}
    }

    # WL Extended Log Format
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{WL_IO_EXTENDED}"}
      add_field => { "log" => "Access Log" }
      add_field => { "server type" => "webserv" }
      remove_tag => ["_grokparsefailure"]
    }

    if [tag] == "_grokparsefailure" {
        grok{
          match => { "message" => "%{COMMONAPACHELOG}"}
          add_field => { "log" => "Access Log" }
          add_field => { "server type" => "webserv" }
          remove_tag => ["_grokparsefailure"]
        }
    }

    if [request] {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => {"request" => "%{PS_URI_REQUEST}"}
        add_tag => ["PIA_reqeust"]
        remove_tag => ["_grokparsefailure"]
      }
    }

    mutate {
      convert => {"duration" => "float"}
      convert => {"bytes" => "integer"}
    }

    kv {
      source => "request"
      target => "parameters"
      field_split => "&?"
      include_keys => ["SEARCH_GROUP", "PAGE"]
    }

    # kv {
    #   source => "cookies"
    #   target => "cookie"
    #   field_split => ";"
    #   value_split => "="
    #   trimkey => "\s"
    # }

    # Fix for tab-delimted time field... build a new field in a friendly format
    mutate {
      add_field => {
        "datetime" => "%{year}-%{month}-%{day} %{time}"
      }
    }

    date {
      match => ["datetime", "yyyy-MM-dd HH:mm:ss"]
      remove_field => [ "datetime" ]
    }
  }


  # if [document_type] == "servlet" {
  #   # drop comment lines
  #   if ([message] =~ /^#/) {
  #     drop{}
  #   }
  #   grok {
  #     patterns_dir => "d:\es-logs\\logstash\patterns"
  #     match => { "message" => "%{SERVLET_LOG}"}
  #     add_field => { "log" => "Servlet Log" }
  #     add_field => { "server type" => "webserv" }
  #     remove_tag => ["_grokparsefailure"]
  #   }
  # }
  # # Resolve IP's to Latitude/Longitude
  # # If XForwardedFor is populated, use that IP for the coordinates. ClientIP, in this case, will be the load balancer IP.
  # # Resolve IP's to Latitude/Longitude
  # # If X-Forwarded-For (forwardedFor) is populated, replace the clientip with the forwardedFor.
  # # if [forwardedFor != "-"]  {
  # #   mutate { replace => { "clientip" => "forwardedFor" } }
  # # }
  # if [forwardedFor] =~ /^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$/ {
  #   geoip {
  #     source => "forwardedFor"
  #     target => "geoip"
  #     add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
  #     add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
  #   }
  # }

  # App Server APPSRV Logs
  if [document_type] == "appsrv_log" {
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{APPSRV_LOG}"}
      add_field => { "log" => "App Server Log" }
      add_field => { "server type" => "appserv" }
      remove_tag => ["_grokparsefailure"]
    }

    if [log_message] =~ "authentication" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{LOGIN_DATA}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["login"]
      }
    }

    if [log_message] =~ "SQL" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{BAD_SQL}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["sql_error"]
      }
    }

    if [log_message] =~ "Executing component" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{EXEC_COMP}" }
        add_tag => ["comp_exec"]
      }

      # Create new entry that combines component name with durations
      aggregate {
        task_id => "%{vm}%{pid}%{service_request}"
        code => "map['componentName'] = event.get('componentName')"
        map_action => "create"
      }
    }

    if [log_message] =~ "elapsed time=" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{SERVICE_DUR}" }
        add_tag => ["comp_duration"]
      }

      aggregate {
        task_id => "%{vm}%{pid}%{service_request}"
        code => "event.set('componentName', map['componentName'])"
        map_action => "update"
        end_of_task => true
        timeout => 600
        push_map_as_event_on_timeout => true
        add_tag => ["duration"]
      }
    }

    if [message] =~ "PSPAL" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "message" => "%{CRASH}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["crash"]
      }
    }

    date {
      match => ["datetime", "MM/dd/YY HH:mm:ss", "MM/dd/YY H:mm:ss", "M/dd/YY HH:mm:ss", "M/dd/YY H:mm:ss",  "ISO8601"]
      remove_field => [ "datetime" ]
    }

    mutate {
      convert => {"duration" => "float"}
    }

  }

  if [useragent] {
    useragent {
      source=> "useragent"
      target => "agent"
    }
  }

  if [document_type] == "appsrv_queue_log" {
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{APP_QUEUE}"}
      add_field => { "log" => "App Server Queue" }
      add_field => { "server_type" => "appserver"}
    }

    mutate {
      convert => {"active_processes" => "integer"}
      convert => {"queued_requests" => "integer"}
    }

    date {
      match => ["datetime", "ISO8601"]
      remove_field => [ "datetime" ]
    }
  }
  if [document_type] == "appsrv_request_log" {
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{APP_REQUEST}"}
      add_field => { "log" => "App Server Request" }
      add_field => { "server_type" => "appserver"}
    }

    mutate {
      convert => {"id" => "integer"}
      convert => {"requests_done" => "integer"}
      convert => {"load_done" => "integer"}
    }

    date {
      match => ["datetime", "ISO8601"]
      remove_field => [ "datetime" ]
    }
  }

}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    manage_template => false
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
  }
  #stdout {
  #  codec => "json"
  #}
}
