input {
   beats {
      type => beats
      port => 5044
   }
}

filter {

  # Drop empty log lines
  if [message] == "" {
    drop { }
  }

  # Web Server Access Logs
  if [document_type] == "access_log" {
    # drop comment lines
    if ([message] =~ /^#/) {
      drop{}
    }

    # WL Extended Log Format
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{WL_IO_EXTENDED}"}
      add_field => { "log" => "Access Log" }
      add_field => { "server type" => "webserv" }
      remove_tag => ["_grokparsefailure"]
    }

    if [tag] == "_grokparsefailure" {
        grok{
          match => { "message" => "%{COMMONAPACHELOG}"}
          add_field => { "log" => "Access Log" }
          add_field => { "server type" => "webserv" }
          remove_tag => ["_grokparsefailure"]
        }
    }

    if [request] {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => {"request" => "%{PS_URI_REQUEST}"}
        add_tag => ["PIA_reqeust"]
        remove_tag => ["_grokparsefailure"]
      }
    }

    mutate {
      convert => {"duration" => "float"}
      convert => {"bytes" => "integer"}
    }

    kv {
      source => "request"
      target => "parameters"
      field_split => "&?"
      include_keys => ["SEARCH_GROUP", "PAGE"]
    }

    # Fix for tab-delimted time field... build a new field in a friendly format
    mutate {
      add_field => {
        "datetime" => "%{year}-%{month}-%{day} %{time}"
      }
    }

    date {
      match => ["datetime", "yyyy-MM-dd HH:mm:ss"]
      remove_field => [ "datetime" ]
    }
  }

  # App Server APPSRV Logs
  if [document_type] == "appsrv_log" {
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{APPSRV_LOG}"}
      add_field => { "log" => "App Server Log" }
      add_field => { "server type" => "appserv" }
      remove_tag => ["_grokparsefailure"]
    }

    if [log_message] =~ "authentication" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{LOGIN_DATA}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["login"]
      }
    }

    if [log_message] =~ "SQL" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{BAD_SQL}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["sql_error"]
      }
    }

    if [log_message] =~ "Executing component" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{EXEC_COMP}" }
        add_tag => ["comp_exec"]
      }

      # Create new entry that combines component name with durations
      aggregate {
        task_id => "%{vm}%{pid}%{service_request}"
        code => "map['componentName'] = event.get('componentName')"
        map_action => "create"
      }
    }

    if [log_message] =~ "elapsed time=" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "log_message" => "%{SERVICE_DUR}" }
        add_tag => ["comp_duration"]
      }

      aggregate {
        task_id => "%{vm}%{pid}%{service_request}"
        code => "event.set('componentName', map['componentName'])"
        map_action => "update"
        end_of_task => true
        timeout => 600
        push_map_as_event_on_timeout => true
        add_tag => ["duration"]
      }
    }

    if [message] =~ "PSPAL" {
      grok {
        patterns_dir => "/usr/share/logstash/pipeline/patterns"
        match => { "message" => "%{CRASH}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["crash"]
      }
    }

    date {
      match => ["datetime", "MM/dd/YY HH:mm:ss", "MM/dd/YY H:mm:ss", "M/dd/YY HH:mm:ss", "M/dd/YY H:mm:ss",  "ISO8601"]
      remove_field => [ "datetime" ]
    }

    mutate {
      convert => {"duration" => "float"}
    }

  }

  if [useragent] {
    useragent {
      source=> "useragent"
      target => "agent"
    }
  }

  if [document_type] == "appsrv_queue_log" {
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{APP_QUEUE}"}
      add_field => { "log" => "App Server Queue" }
      add_field => { "server_type" => "appserver"}
    }

    mutate {
      convert => {"active_processes" => "integer"}
      convert => {"queued_requests" => "integer"}
    }

    date {
      match => ["datetime", "ISO8601"]
      remove_field => [ "datetime" ]
    }
  }
  if [document_type] == "appsrv_request_log" {
    grok {
      patterns_dir => "/usr/share/logstash/pipeline/patterns"
      match => { "message" => "%{APP_REQUEST}"}
      add_field => { "log" => "App Server Request" }
      add_field => { "server_type" => "appserver"}
    }

    mutate {
      convert => {"id" => "integer"}
      convert => {"requests_done" => "integer"}
      convert => {"load_done" => "integer"}
    }

    date {
      match => ["datetime", "ISO8601"]
      remove_field => [ "datetime" ]
    }
  }

}

output {
  elasticsearch {
    hosts           => ["https://opensearch-node1:9200"]
    user            => "admin"
    password        => "admin"
    index           => "pslogs-%{+YYYY-MM-dd}"
    action          => create
    manage_template => false
    ssl             => true
    ssl_certificate_verification => false
  }
  #stdout {
  #  codec => "json"
  #}
}
